{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518eb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ElectricityAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Detects anomalies in electricity consumption data using multiple methods:\n",
    "    1. Statistical methods (Z-score, IQR)\n",
    "    2. Isolation Forest (machine learning)\n",
    "    3. Time-based patterns\n",
    "    \n",
    "    Supports:\n",
    "    - UCI Household Electric Power Consumption dataset\n",
    "    - Kaggle Smart Meter dataset\n",
    "    - Custom datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, contamination=0.05):\n",
    "        \"\"\"\n",
    "        Initialize the anomaly detector\n",
    "        \n",
    "        Parameters:\n",
    "        contamination (float): Expected proportion of anomalies (default 5%)\n",
    "        \"\"\"\n",
    "        self.contamination = contamination\n",
    "        self.scaler = StandardScaler()\n",
    "        self.iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "        \n",
    "    def load_uci_dataset(self, filepath='household_power_consumption.txt'):\n",
    "        \"\"\"\n",
    "        Load UCI Household Electric Power Consumption dataset\n",
    "        \n",
    "        Dataset info:\n",
    "        - Separator: semicolon (;)\n",
    "        - Date format: dd/mm/yyyy\n",
    "        - Time format: hh:mm:ss\n",
    "        - Missing values: '?' or empty\n",
    "        - Main consumption column: Global_active_power (kilowatt)\n",
    "        \n",
    "        Parameters:\n",
    "        filepath (str): Path to the .txt file\n",
    "        \"\"\"\n",
    "        print(\"Loading UCI Household Electric Power Consumption dataset...\")\n",
    "        \n",
    "        # Load with semicolon separator, replace '?' with NaN\n",
    "        self.df = pd.read_csv(filepath, \n",
    "                             sep=';', \n",
    "                             na_values=['?', ''],\n",
    "                             low_memory=False)\n",
    "        \n",
    "        # Combine Date and Time columns into datetime\n",
    "        self.df['datetime'] = pd.to_datetime(\n",
    "            self.df['Date'] + ' ' + self.df['Time'], \n",
    "            format='%d/%m/%Y %H:%M:%S'\n",
    "        )\n",
    "        \n",
    "        # Set consumption column\n",
    "        self.consumption_col = 'Global_active_power'\n",
    "        self.date_col = 'datetime'\n",
    "        \n",
    "        # Convert consumption to numeric\n",
    "        self.df[self.consumption_col] = pd.to_numeric(\n",
    "            self.df[self.consumption_col], \n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # Sort by datetime\n",
    "        self.df = self.df.sort_values('datetime').reset_index(drop=True)\n",
    "        \n",
    "        # Remove rows where consumption is NaN\n",
    "        initial_len = len(self.df)\n",
    "        self.df = self.df.dropna(subset=[self.consumption_col])\n",
    "        \n",
    "        print(f\"✓ Data loaded: {len(self.df):,} records\")\n",
    "        print(f\"  Removed {initial_len - len(self.df):,} rows with missing consumption\")\n",
    "        print(f\"  Date range: {self.df['datetime'].min()} to {self.df['datetime'].max()}\")\n",
    "        print(f\"  Consumption column: {self.consumption_col} (kilowatt)\")\n",
    "        print(f\"\\nAvailable columns: {', '.join(self.df.columns)}\")\n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        return self.df.head()\n",
    "    \n",
    "    def load_kaggle_dataset(self, filepath='household_power_consumption.csv'):\n",
    "        \"\"\"\n",
    "        Load Kaggle Smart Meter dataset (CSV version)\n",
    "        \n",
    "        This is often the same as UCI but in CSV format\n",
    "        \n",
    "        Parameters:\n",
    "        filepath (str): Path to the .csv file\n",
    "        \"\"\"\n",
    "        print(\"Loading Kaggle Smart Meter dataset...\")\n",
    "        \n",
    "        self.df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Try to identify datetime column\n",
    "        datetime_candidates = ['datetime', 'Datetime', 'timestamp', 'Date']\n",
    "        for col in datetime_candidates:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = pd.to_datetime(self.df[col])\n",
    "                self.df = self.df.sort_values(col)\n",
    "                self.date_col = col\n",
    "                break\n",
    "        \n",
    "        # Try to identify consumption column\n",
    "        consumption_candidates = ['Global_active_power', 'global_active_power', \n",
    "                                 'consumption', 'power', 'kwh', 'usage']\n",
    "        for col in self.df.columns:\n",
    "            if any(candidate in col for candidate in consumption_candidates):\n",
    "                self.consumption_col = col\n",
    "                break\n",
    "        \n",
    "        # Convert consumption to numeric\n",
    "        self.df[self.consumption_col] = pd.to_numeric(\n",
    "            self.df[self.consumption_col], \n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # Remove rows where consumption is NaN\n",
    "        initial_len = len(self.df)\n",
    "        self.df = self.df.dropna(subset=[self.consumption_col])\n",
    "        \n",
    "        print(f\"✓ Data loaded: {len(self.df):,} records\")\n",
    "        print(f\"  Removed {initial_len - len(self.df):,} rows with missing consumption\")\n",
    "        if hasattr(self, 'date_col'):\n",
    "            print(f\"  Date range: {self.df[self.date_col].min()} to {self.df[self.date_col].max()}\")\n",
    "        print(f\"  Consumption column: {self.consumption_col}\")\n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        return self.df.head()\n",
    "    \n",
    "    def load_custom_data(self, filepath, date_column=None, consumption_column=None, \n",
    "                        separator=',', date_format=None):\n",
    "        \"\"\"\n",
    "        Load custom electricity consumption data\n",
    "        \n",
    "        Parameters:\n",
    "        filepath (str): Path to CSV/TXT file\n",
    "        date_column (str): Name of datetime column (optional)\n",
    "        consumption_column (str): Name of consumption column (required)\n",
    "        separator (str): Column separator (default: ',')\n",
    "        date_format (str): Date format string (e.g., '%Y-%m-%d %H:%M:%S')\n",
    "        \"\"\"\n",
    "        print(\"Loading custom dataset...\")\n",
    "        \n",
    "        self.df = pd.read_csv(filepath, sep=separator)\n",
    "        \n",
    "        if date_column:\n",
    "            if date_format:\n",
    "                self.df[date_column] = pd.to_datetime(self.df[date_column], format=date_format)\n",
    "            else:\n",
    "                self.df[date_column] = pd.to_datetime(self.df[date_column])\n",
    "            self.df = self.df.sort_values(date_column)\n",
    "            self.date_col = date_column\n",
    "        \n",
    "        if consumption_column:\n",
    "            self.consumption_col = consumption_column\n",
    "        else:\n",
    "            print(\"ERROR: consumption_column must be specified for custom data\")\n",
    "            return None\n",
    "        \n",
    "        # Convert consumption to numeric\n",
    "        self.df[self.consumption_col] = pd.to_numeric(\n",
    "            self.df[self.consumption_col], \n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # Remove rows where consumption is NaN\n",
    "        initial_len = len(self.df)\n",
    "        self.df = self.df.dropna(subset=[self.consumption_col])\n",
    "        \n",
    "        print(f\"✓ Data loaded: {len(self.df):,} records\")\n",
    "        print(f\"  Removed {initial_len - len(self.df):,} rows with missing consumption\")\n",
    "        print(f\"  Consumption column: {self.consumption_col}\")\n",
    "        return self.df.head()\n",
    "    \n",
    "    def detect_zscore(self, threshold=3):\n",
    "        \"\"\"\n",
    "        Detect anomalies using Z-score method\n",
    "        Flags values more than 'threshold' standard deviations from mean\n",
    "        \"\"\"\n",
    "        z_scores = np.abs(stats.zscore(self.df[self.consumption_col].dropna()))\n",
    "        self.df['anomaly_zscore'] = False\n",
    "        self.df.loc[self.df[self.consumption_col].notna(), 'anomaly_zscore'] = z_scores > threshold\n",
    "        \n",
    "        n_anomalies = self.df['anomaly_zscore'].sum()\n",
    "        print(f\"Z-score method: {n_anomalies:,} anomalies detected ({n_anomalies/len(self.df)*100:.2f}%)\")\n",
    "        return self.df['anomaly_zscore']\n",
    "    \n",
    "    def detect_iqr(self, multiplier=1.5):\n",
    "        \"\"\"\n",
    "        Detect anomalies using Interquartile Range (IQR) method\n",
    "        Flags values outside [Q1 - multiplier*IQR, Q3 + multiplier*IQR]\n",
    "        \"\"\"\n",
    "        Q1 = self.df[self.consumption_col].quantile(0.25)\n",
    "        Q3 = self.df[self.consumption_col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "        \n",
    "        self.df['anomaly_iqr'] = (\n",
    "            (self.df[self.consumption_col] < lower_bound) | \n",
    "            (self.df[self.consumption_col] > upper_bound)\n",
    "        )\n",
    "        \n",
    "        n_anomalies = self.df['anomaly_iqr'].sum()\n",
    "        print(f\"IQR method: {n_anomalies:,} anomalies detected ({n_anomalies/len(self.df)*100:.2f}%)\")\n",
    "        print(f\"Normal range: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        return self.df['anomaly_iqr']\n",
    "    \n",
    "    def detect_isolation_forest(self):\n",
    "        \"\"\"\n",
    "        Detect anomalies using Isolation Forest algorithm\n",
    "        Works well for high-dimensional data and doesn't assume data distribution\n",
    "        \"\"\"\n",
    "        # Prepare features\n",
    "        features = self.df[[self.consumption_col]].dropna()\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Predict anomalies (-1 for anomaly, 1 for normal)\n",
    "        predictions = self.iso_forest.fit_predict(features_scaled)\n",
    "        \n",
    "        self.df['anomaly_iforest'] = False\n",
    "        self.df.loc[features.index, 'anomaly_iforest'] = predictions == -1\n",
    "        \n",
    "        n_anomalies = self.df['anomaly_iforest'].sum()\n",
    "        print(f\"Isolation Forest: {n_anomalies:,} anomalies detected ({n_anomalies/len(self.df)*100:.2f}%)\")\n",
    "        return self.df['anomaly_iforest']\n",
    "    \n",
    "    def detect_time_based(self):\n",
    "        \"\"\"\n",
    "        Detect anomalies based on time patterns (hourly/daily patterns)\n",
    "        Requires datetime column\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'date_col'):\n",
    "            print(\"Time-based detection requires a datetime column\")\n",
    "            return None\n",
    "        \n",
    "        # Extract time features\n",
    "        self.df['hour'] = self.df[self.date_col].dt.hour\n",
    "        self.df['day_of_week'] = self.df[self.date_col].dt.dayofweek\n",
    "        \n",
    "        # Calculate mean and std for each hour\n",
    "        hourly_stats = self.df.groupby('hour')[self.consumption_col].agg(['mean', 'std'])\n",
    "        \n",
    "        # Detect anomalies based on hourly patterns\n",
    "        self.df['anomaly_time'] = False\n",
    "        for hour in range(24):\n",
    "            mask = self.df['hour'] == hour\n",
    "            if mask.sum() > 0 and not pd.isna(hourly_stats.loc[hour, 'std']):\n",
    "                mean_val = hourly_stats.loc[hour, 'mean']\n",
    "                std_val = hourly_stats.loc[hour, 'std']\n",
    "                \n",
    "                anomalies = np.abs(self.df.loc[mask, self.consumption_col] - mean_val) > (3 * std_val)\n",
    "                self.df.loc[mask, 'anomaly_time'] = anomalies\n",
    "        \n",
    "        n_anomalies = self.df['anomaly_time'].sum()\n",
    "        print(f\"Time-based method: {n_anomalies:,} anomalies detected ({n_anomalies/len(self.df)*100:.2f}%)\")\n",
    "        return self.df['anomaly_time']\n",
    "    \n",
    "    def detect_all(self):\n",
    "        \"\"\"\n",
    "        Run all anomaly detection methods and create consensus\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RUNNING ALL ANOMALY DETECTION METHODS\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        self.detect_zscore()\n",
    "        self.detect_iqr()\n",
    "        self.detect_isolation_forest()\n",
    "        \n",
    "        if hasattr(self, 'date_col'):\n",
    "            self.detect_time_based()\n",
    "            # Consensus: flagged by at least 2 methods\n",
    "            self.df['anomaly_consensus'] = (\n",
    "                self.df['anomaly_zscore'].astype(int) + \n",
    "                self.df['anomaly_iqr'].astype(int) + \n",
    "                self.df['anomaly_iforest'].astype(int) + \n",
    "                self.df['anomaly_time'].astype(int)\n",
    "            ) >= 2\n",
    "        else:\n",
    "            # Consensus: flagged by at least 2 methods\n",
    "            self.df['anomaly_consensus'] = (\n",
    "                self.df['anomaly_zscore'].astype(int) + \n",
    "                self.df['anomaly_iqr'].astype(int) + \n",
    "                self.df['anomaly_iforest'].astype(int)\n",
    "            ) >= 2\n",
    "        \n",
    "        n_consensus = self.df['anomaly_consensus'].sum()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CONSENSUS ANOMALIES: {n_consensus:,} ({n_consensus/len(self.df)*100:.2f}%)\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def get_anomalies(self, method='consensus'):\n",
    "        \"\"\"\n",
    "        Get anomalous records\n",
    "        \n",
    "        Parameters:\n",
    "        method (str): 'consensus', 'zscore', 'iqr', 'iforest', or 'time'\n",
    "        \"\"\"\n",
    "        col_name = f'anomaly_{method}'\n",
    "        if col_name not in self.df.columns:\n",
    "            print(f\"Method '{method}' not found. Run detection first.\")\n",
    "            return None\n",
    "        \n",
    "        return self.df[self.df[col_name] == True]\n",
    "    \n",
    "    def visualize(self, sample_size=10000):\n",
    "        \"\"\"\n",
    "        Visualize anomalies in the data\n",
    "        \"\"\"\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        \n",
    "        # Sample data if too large\n",
    "        df_plot = self.df.sample(min(sample_size, len(self.df)))\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "        fig.suptitle('Electricity Consumption Anomaly Detection Results', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Consumption over time\n",
    "        ax1 = axes[0, 0]\n",
    "        if hasattr(self, 'date_col'):\n",
    "            normal = df_plot[~df_plot['anomaly_consensus']]\n",
    "            anomaly = df_plot[df_plot['anomaly_consensus']]\n",
    "            \n",
    "            ax1.scatter(normal[self.date_col], normal[self.consumption_col], \n",
    "                       c='steelblue', alpha=0.3, s=1, label='Normal', rasterized=True)\n",
    "            ax1.scatter(anomaly[self.date_col], anomaly[self.consumption_col], \n",
    "                       c='red', alpha=0.8, s=15, label='Anomaly', zorder=5)\n",
    "            ax1.set_xlabel('Time', fontsize=11)\n",
    "            ax1.set_ylabel('Consumption (kW)', fontsize=11)\n",
    "            ax1.set_title('Consumption Over Time', fontsize=12, fontweight='bold')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax1.plot(df_plot.index, df_plot[self.consumption_col], \n",
    "                    'steelblue', alpha=0.5, linewidth=0.5)\n",
    "            anomalies = df_plot[df_plot['anomaly_consensus']]\n",
    "            ax1.scatter(anomalies.index, anomalies[self.consumption_col], \n",
    "                       c='red', s=15, label='Anomaly', zorder=5)\n",
    "            ax1.set_xlabel('Index', fontsize=11)\n",
    "            ax1.set_ylabel('Consumption (kW)', fontsize=11)\n",
    "            ax1.set_title('Consumption Pattern', fontsize=12, fontweight='bold')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Distribution with anomalies\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.hist(df_plot[self.consumption_col], bins=50, alpha=0.6, \n",
    "                color='steelblue', label='All data', edgecolor='black')\n",
    "        anomalies = df_plot[df_plot['anomaly_consensus']]\n",
    "        ax2.hist(anomalies[self.consumption_col], bins=50, alpha=0.8, \n",
    "                color='red', label='Anomalies', edgecolor='darkred')\n",
    "        ax2.set_xlabel('Consumption (kW)', fontsize=11)\n",
    "        ax2.set_ylabel('Frequency', fontsize=11)\n",
    "        ax2.set_title('Distribution of Consumption', fontsize=12, fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Plot 3: Method comparison\n",
    "        ax3 = axes[1, 0]\n",
    "        methods = ['zscore', 'iqr', 'iforest']\n",
    "        method_labels = ['Z-Score', 'IQR', 'Iso Forest']\n",
    "        \n",
    "        if 'anomaly_time' in self.df.columns:\n",
    "            methods.append('time')\n",
    "            method_labels.append('Time-based')\n",
    "        \n",
    "        counts = [self.df[f'anomaly_{m}'].sum() for m in methods]\n",
    "        colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12'][:len(methods)]\n",
    "        \n",
    "        bars = ax3.bar(method_labels, counts, color=colors, edgecolor='black', linewidth=1.5)\n",
    "        ax3.set_ylabel('Number of Anomalies', fontsize=11)\n",
    "        ax3.set_title('Anomalies by Detection Method', fontsize=12, fontweight='bold')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height):,}',\n",
    "                    ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # Plot 4: Box plot comparison\n",
    "        ax4 = axes[1, 1]\n",
    "        normal_data = df_plot[~df_plot['anomaly_consensus']][self.consumption_col].dropna()\n",
    "        anomaly_data = df_plot[df_plot['anomaly_consensus']][self.consumption_col].dropna()\n",
    "        \n",
    "        bp = ax4.boxplot([normal_data, anomaly_data], \n",
    "                         labels=['Normal', 'Anomaly'],\n",
    "                         patch_artist=True,\n",
    "                         notch=True,\n",
    "                         showfliers=False)\n",
    "        \n",
    "        bp['boxes'][0].set_facecolor('steelblue')\n",
    "        bp['boxes'][1].set_facecolor('red')\n",
    "        \n",
    "        ax4.set_ylabel('Consumption (kW)', fontsize=11)\n",
    "        ax4.set_title('Consumption Distribution: Normal vs Anomaly', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('anomaly_detection_results.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"✓ Visualization saved as 'anomaly_detection_results.png'\")\n",
    "        plt.show()\n",
    "    \n",
    "    def summary_report(self):\n",
    "        \"\"\"\n",
    "        Generate a summary report of anomaly detection\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" \"*15 + \"ANOMALY DETECTION SUMMARY REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\n{'Dataset Information:':<30}\")\n",
    "        print(f\"  {'Total records:':<28} {len(self.df):,}\")\n",
    "        print(f\"  {'Consumption column:':<28} {self.consumption_col}\")\n",
    "        if hasattr(self, 'date_col'):\n",
    "            print(f\"  {'Date range:':<28} {self.df[self.date_col].min().strftime('%Y-%m-%d')} to {self.df[self.date_col].max().strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        print(f\"\\n{'Consumption Statistics:':<30}\")\n",
    "        print(f\"  {'Mean:':<28} {self.df[self.consumption_col].mean():.3f} kW\")\n",
    "        print(f\"  {'Median:':<28} {self.df[self.consumption_col].median():.3f} kW\")\n",
    "        print(f\"  {'Std Dev:':<28} {self.df[self.consumption_col].std():.3f} kW\")\n",
    "        print(f\"  {'Min:':<28} {self.df[self.consumption_col].min():.3f} kW\")\n",
    "        print(f\"  {'Max:':<28} {self.df[self.consumption_col].max():.3f} kW\")\n",
    "        \n",
    "        print(f\"\\n{'Anomaly Detection Results:':<30}\")\n",
    "        for method in ['zscore', 'iqr', 'iforest', 'time', 'consensus']:\n",
    "            col = f'anomaly_{method}'\n",
    "            if col in self.df.columns:\n",
    "                count = self.df[col].sum()\n",
    "                pct = count / len(self.df) * 100\n",
    "                method_name = method.upper().replace('_', '-')\n",
    "                print(f\"  {method_name + ':':<28} {count:,} anomalies ({pct:.2f}%)\")\n",
    "        \n",
    "        # Show top anomalies\n",
    "        if 'anomaly_consensus' in self.df.columns:\n",
    "            print(f\"\\n{'Top 10 Anomalous Consumption Values:':<30}\")\n",
    "            top_anomalies = self.df[self.df['anomaly_consensus']].nlargest(10, self.consumption_col)\n",
    "            for i, (idx, row) in enumerate(top_anomalies.iterrows(), 1):\n",
    "                timestamp = \"\"\n",
    "                if hasattr(self, 'date_col'):\n",
    "                    timestamp = f\" at {row[self.date_col].strftime('%Y-%m-%d %H:%M')}\"\n",
    "                print(f\"  {i:2d}. {row[self.consumption_col]:8.3f} kW{timestamp}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE FOR BOTH DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "def example_uci_dataset():\n",
    "    \"\"\"Example: Load and analyze UCI dataset\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \"*20 + \"UCI DATASET EXAMPLE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize detector\n",
    "    detector = ElectricityAnomalyDetector(contamination=0.05)\n",
    "    \n",
    "    # Load UCI dataset\n",
    "    detector.load_uci_dataset('household_power_consumption.txt')\n",
    "    \n",
    "    # Run all detection methods\n",
    "    detector.detect_all()\n",
    "    \n",
    "    # Get consensus anomalies\n",
    "    anomalies = detector.get_anomalies(method='consensus')\n",
    "    print(f\"\\nFound {len(anomalies):,} consensus anomalies\")\n",
    "    \n",
    "    # Visualize results\n",
    "    detector.visualize(sample_size=10000)\n",
    "    \n",
    "    # Generate summary report\n",
    "    detector.summary_report()\n",
    "    \n",
    "    # Export anomalies to CSV\n",
    "    anomalies.to_csv('uci_anomalies.csv', index=False)\n",
    "    print(\"✓ Anomalies exported to 'uci_anomalies.csv'\\n\")\n",
    "    \n",
    "    return detector, anomalies\n",
    "\n",
    "\n",
    "def example_kaggle_dataset():\n",
    "    \"\"\"Example: Load and analyze Kaggle dataset\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \"*20 + \"KAGGLE DATASET EXAMPLE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize detector\n",
    "    detector = ElectricityAnomalyDetector(contamination=0.05)\n",
    "    \n",
    "    # Load Kaggle dataset\n",
    "    detector.load_kaggle_dataset('household_power_consumption.csv')\n",
    "    \n",
    "    # Run all detection methods\n",
    "    detector.detect_all()\n",
    "    \n",
    "    # Get consensus anomalies\n",
    "    anomalies = detector.get_anomalies(method='consensus')\n",
    "    \n",
    "    # Visualize results\n",
    "    detector.visualize(sample_size=10000)\n",
    "    \n",
    "    # Generate summary report\n",
    "    detector.summary_report()\n",
    "    \n",
    "    # Export anomalies\n",
    "    anomalies.to_csv('kaggle_anomalies.csv', index=False)\n",
    "    print(\"✓ Anomalies exported to 'kaggle_anomalies.csv'\\n\")\n",
    "    \n",
    "    return detector, anomalies\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \"*15 + \"ELECTRICITY ANOMALY DETECTOR\")\n",
    "    print(\" \"*25 + \"Ready to Use!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nTo use this script:\")\n",
    "    print(\"\\n1. For UCI dataset (household_power_consumption.txt):\")\n",
    "    print(\"   detector, anomalies = example_uci_dataset()\")\n",
    "    print(\"\\n2. For Kaggle dataset (household_power_consumption.csv):\")\n",
    "    print(\"   detector, anomalies = example_kaggle_dataset()\")\n",
    "    print(\"\\n3. Or uncomment one of the examples above and run the script!\")\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Uncomment one of these to run:\n",
    "    detector, anomalies = example_uci_dataset()\n",
    "    detector, anomalies = example_kaggle_dataset()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
